{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15hNIMO5M16Llh5tYoNrp0C8-HKuPUSOu",
      "authorship_tag": "ABX9TyPUh1/T3wk1ovXul4VDHbk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batsoyla/E6.5_2016-17/blob/master/Bert_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WLZkHEMrGkw",
        "colab_type": "text"
      },
      "source": [
        "Πηγαίνω στον φάκελο που έχω αποθηκεύσει τον φάκελο File_sent_analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck7lIfyFAjqB",
        "colab_type": "code",
        "outputId": "cb1cc931-b3e6-45ae-f469-cb5e85ddbe6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd/content/drive/My Drive/MyThesis/File_sent_analysis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MyThesis/File_sent_analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJhHKd-1N8Cb",
        "colab_type": "code",
        "outputId": "92b7ab72-2248-46b1-8167-64f6fab6cd72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n",
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.29.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (46.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b98dfc6a2a341ca690b0f502e605287be9caed574b00a543c79e248ab7248eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV-mthWVcnN2",
        "colab_type": "text"
      },
      "source": [
        "# **ΑΡΧΙΚΟ ΔΕΙΓΜΑ ΔΕΔΟΜΕΝΩΝ**\n",
        "\n",
        "Αρχικά εντοπίζεται το path και φορτώνεται ολόκληρο το δείγμα δεδομένων, διαγραφονται οι στήλες του πίνακα, μετονομάζονται κατάλληλα και γίνονται κάποιες αρχικές οπτικοποιήσεις, για να έχουμε μια πρώτη πολύ πρώιμη εικόνα των δεδομένων. Τα δεδομένα αρχικά βρίσκονται στον φάκελο Crawled_Data και είναι σε μορφή Json αρχείων.Επιλέχθηκε να πραγματοποιηθεί binary sentiment analysis και ετσι όσες κριτικές έλαβαν 4 ή 5 αστεράκια να θεωρηθούν θετικές και όσες έλαβαν 2 ή 1 να θεωρηθούν αρνητικές και να μην χρησιμοποιηθούν καθόλου ουδέτερες κριτικές."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeMgw0vpMPCH",
        "colab_type": "code",
        "outputId": "9d83d02e-0adc-4958-9d1a-65f1789d1a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "#from Data_Process import dictionaries\n",
        "import dictionaries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tabulate import _table_formats, tabulate\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCAzMuwVdKbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge the given datafiles\n",
        "def merge_to_dataframe_save(path):\n",
        "  list_of_dataframes=[]\n",
        "  list_crawled_data_files=os.listdir(path)\n",
        "  a=os.path.join(path, list_crawled_data_files[0])\n",
        "#erase file if it exists:\n",
        "  for item in list_crawled_data_files:\n",
        "    if os.path.isfile(os.path.join(path, item)):\n",
        "      if os.path.join(path, item)[:65] != \"/content/drive/My Drive/MyThesis/Thesis/Crawled_Data/TripAdvisor_\":\n",
        "        os.remove(os.path.join(path, item))\n",
        "      else:\n",
        "        df_temp=pd.read_json(os.path.join(path, item), lines = True)\n",
        "        list_of_dataframes.append(df_temp)\n",
        "  df=pd.concat(list_of_dataframes,  ignore_index=True)\n",
        "  df.to_json(os.path.join(path, \"Crawled_Data.json\"), orient=\"records\", lines=True)\n",
        "  return df\n",
        "\n",
        "#create polarity column\n",
        "def pol_column(value):\n",
        "  if value > 3:\n",
        "    return 1\n",
        "  elif value < 3:\n",
        "    return 0\n",
        "\n",
        "#create sentiment column\n",
        "def sentiment_column(n):\n",
        "  switcher = {\n",
        "      1: \"Positive\",\n",
        "      0: \"Negative\" \n",
        "    }\n",
        "  return switcher.get(n, \"Invalid data\")\n",
        "\n",
        "#flatten list of lists\n",
        "def flatten(list_of_lists):\n",
        "  for sublist in list_of_lists:\n",
        "    string=\"\".join(sublist)\n",
        "  return string\n",
        "\n",
        "def sent_pol_table(data):\n",
        "  table = [[\"Negative\", 0, len(data[data[\"Sentiment\"] ==\"Negative\"])],[\"Positive\", 1, len(data[data[\"Sentiment\"] ==\"Positive\"])]]\n",
        "  headers = [\"Sentiment\", \"Polarity\",\"No_rows\"]\n",
        "  print(tabulate(table, headers, tablefmt=\"psql\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mOqbPGhvvQF",
        "colab_type": "code",
        "outputId": "b270176c-751c-4db5-f694-8ecdb9806ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "#path retrieval\n",
        "crawled_data_path = os.path.abspath(\"Crawled_Data\")\n",
        "\n",
        "# merge dataframe\n",
        "df=merge_to_dataframe_save(crawled_data_path)\n",
        "\n",
        "#create polarity column\n",
        "df[\"Polarity\"]=df[\"Star\"].apply(pol_column)\n",
        "\n",
        "# create sentiment column\n",
        "df[\"Sentiment\"]=df[\"Polarity\"].apply(sentiment_column)\n",
        "\n",
        "#drop columns that we don't need\n",
        "df = df.drop([\"Review_title\", \"Rating_Date\", \"Restaurant_Name\", \"Star\"], axis=1)\n",
        "\n",
        "#flatten list of lists\n",
        "df[\"Review\"]=df[\"Review\"].apply(flatten)\n",
        "\n",
        "#Drop Null/NA Values from DataFrame\n",
        "df=df.dropna()\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "#drop duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "#print occurencies per class\n",
        "sent_pol_table(df)\n",
        "sns.countplot(x=\"Polarity\", data=df, palette=\"husl\")\n",
        "plt.xlabel(\"Review Score\")\n",
        "plt.title(\"Unbalanced Dataset\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-58033eafa4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# merge dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_to_dataframe_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawled_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#create polarity column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f365930f523b>\u001b[0m in \u001b[0;36mmerge_to_dataframe_save\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlist_of_dataframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlist_crawled_data_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_crawled_data_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#erase file if it exists:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_crawled_data_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk4amAaNjAyK",
        "colab_type": "text"
      },
      "source": [
        "# **ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΔΕΔΟΜΕΝΩΝ**\n",
        "***ΔΗΜΙΟΥΡΓΙΑ ΙΣΟΡΡΟΠΗΜΕΝΟΥ ΔΕΙΓΜΑΤΟΣ***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Από τις παραπάνω οπτικοποιήσεις γίνεται προφανές ότι το δείγμα δεδομένων παρουσιάζει αισθητή ανισορροπία κάτι που δυνητικά επηρεάζει την εκπάιδευση του μοντέλου, το μοντέλο πολύ πιθανόν να \"προβλέπει\" την συχνότερη κλάση χωρίς την ανάλογη ανάλυση δίνοντας έτσι διαστρευλωμένα αποτελέσματα ακρίβειας πρόβλεψης. Για την εξισορρόπηση του δείγματος επιλέγεται η τεχνική του *undersampling* όπου απο ένα πολύ μεγαλύτερο δείγμα επιλέγεται να αφαιρεθούν γραμμές από τις επικρατούσες κλάσεις, οδηγώντας έτσι σε ένα μικρότερο αλλά ισορροπιμένο δείγμα συνολικά **print(df.shape[0])** γραμμών."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG1dRflijGKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#occurencies of min class\n",
        "min_occ=df.groupby(\"Polarity\")[\"Sentiment\"].count().min()\n",
        "\n",
        "#shuffle the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "target_count = df.Sentiment.value_counts()\n",
        "\n",
        "# devide dataset by class\n",
        "df_pos = df[df[\"Polarity\"] == 1]\n",
        "df_neg = df[df[\"Polarity\"] == 0]\n",
        "\n",
        "# Random under-sampling\n",
        "df_pos = df_pos.sample(min_occ)\n",
        "df_neg = df_neg.sample(min_occ)\n",
        "\n",
        "#concatenate to new dataset re-shuffle and re-index data\n",
        "df_under = pd.concat([df_pos, df_neg], axis=0).sample(frac=1)\n",
        "\n",
        "#we will keep the rest of the dataframe to make predictions (left_join)\n",
        "df_pred = df.merge(df_under, on=[\"Review\"], how='left', indicator=True)\n",
        "df_pred = df_pred[df_pred[\"_merge\"] == \"left_only\"]\n",
        "df_pred = df_pred.drop([\"_merge\",\"Polarity_y\", \"Sentiment_y\"], axis=1)\n",
        "df_pred =df_pred.reset_index(drop=True)\n",
        "df_pred.rename(columns={\"Polarity_x\": \"Polarity\", \"Sentiment_x\":\"Sentiment\"}, inplace=True)\n",
        "df_under=df_under.reset_index(drop=True)\n",
        "\n",
        "#save the final dataset\n",
        "df_under.to_json(os.path.join(crawled_data_path, 'Data_to_Process.json'), orient='records', lines=True)\n",
        "\n",
        "#print new dataset\n",
        "sent_pol_table(df_under)\n",
        "sns.countplot(x=\"Sentiment\", data=df_under, palette=\"husl\")\n",
        "plt.xlabel(\"Review Score\")\n",
        "plt.title(\"New Balanced Dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1sEDu_Uk8dl",
        "colab_type": "text"
      },
      "source": [
        "# ***ΚΑΘΑΡΙΣΜΟΣ ΔΕΙΓΜΑΤΟΣ*** \n",
        "\n",
        "\n",
        "---\n",
        "Αρχικά επιλέχθηκέ να γίνει μια προεπεξεργασία του δείγματος που στόχο έχει την διάνθισει της πληροφορία του δείγματος και την ορθότερη απόδοση του, χωρίς ορθογραφικά λάθη και άχρηστες πληροφορίες που μόνο \"θόρυβο\" προκαλούν στο δείγμα μας.\n",
        "Για τον καθαρισμό των δεδομένων μας επιλέχθηκαν απλές τεχνικές όπως\n",
        "\n",
        "1.   Εξάληψη επιπλέον κενών \n",
        "2.   Απόδοση σύντομεύσεων σε πλήρες κείμενο \n",
        "3. Αντικάτάσταση emoji/emoticons με αντιπροσωπευτηκές λέξης\n",
        "4. Διαγραφή URL\n",
        "5. Ορθογραφικός έλεγχος \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V8cMGjur7Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAwRsWggqxQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove url\n",
        "df_under[\"Review\"]=df_under[\"Review\"].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "#lowercase and remove extra blanks,tabs etc\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "# replace repeated characers (only when character is repeated more than two times)\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: ' '.join(re.sub(r\"(.)\\1+\", r'\\1\\1', item) for item in x.split()))\n",
        "\n",
        "#replace emojis/emoticons\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: ' '.join(dictionaries.emoticons_emo.get(item) if item  in dictionaries.emoticons_emo else item for item in x.split()))\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: ' '.join(dictionaries.emoticons.get(item) if item  in dictionaries.emoticons else item for item in x.split()))\n",
        "\n",
        "#All the apostrophes to be converted into standard lexicons APPOSTOPHES[word] if word in APPOSTOPHES else word for word in words]\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: ' '.join(dictionaries.appos.get(item) if item  in dictionaries.appos  else item for item in x.split()))\n",
        "\n",
        "#replace slang to actual correct words\n",
        "df_under[\"Review\"]=df_under[\"Review\"].apply(lambda x: ' '.join(dictionaries.slang.get(item) if item  in dictionaries.slang  else item for item in x.split()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfbuJeYMCskp",
        "colab_type": "text"
      },
      "source": [
        "Γίνεται ορθογραφικός έλεγχος ένα βήμα αρκετά χρονοβόρο καθώς χρειάζεται στο δείγμα μας περίπου ... για να ολοκληρωθεί."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-Ni28NtBxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspellchecker\n",
        "import time\n",
        "\n",
        "#correct grammar\n",
        "start_time = time.time()\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "df_under['Review']=df_under['Review'].apply(lambda x: ' '.join(spell.correction(item) for item in x.split()))\n",
        "print(\"Execution time : %s\" % (time.time() - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6AuyhrUg8Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_under.to_json(os.path.join(crawled_data_path, 'Data_to_Process.json'), orient='records', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBxWonUL9Ecw",
        "colab_type": "text"
      },
      "source": [
        "Ακολουθούν κάποιες οπτικοποιήσεις καθώς και η αφαίρεση κάποιων στήλων από τα δεδομένα που πλέον δεν χρειάζονται."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkIH-UUtM0Z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def round_half_up(n, decimals=0):\n",
        "  multiplier = 10 ** decimals\n",
        "  return math.floor(n*multiplier + 0.5) / multiplier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqUYQphC68s-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tabulate import _table_formats, tabulate\n",
        "\n",
        "#calculate mean of words\n",
        "df_under[\"words_number\"]=df_under[\"Review\"].str.split().str.len()\n",
        "\n",
        "#calculate number of sentences\n",
        "df_under[\"sentences_number\"]=df[\"Review\"].str.split(\".\").str.len()\n",
        "table = [[\"Average number of words\", round_half_up(df_under[\"words_number\"].mean(),2)],[\"Anerage number of sentences\", round_half_up(df_under[\"sentences_number\"].mean(),2)]]\n",
        "print(tabulate(table, tablefmt=\"psql\"))\n",
        "\n",
        "#drop columns\n",
        "df_under = df_under.drop([ \"words_number\", \"sentences_number\"], axis=1)\n",
        "\n",
        "#change the order of columns\n",
        "df_under = df_under[[\"Review\",\"Sentiment\",\"Polarity\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3KQSp5U9sFC",
        "colab_type": "text"
      },
      "source": [
        "# **ΑΝΑΛΥΣΗ ΣΥΝΑΙΣΘΗΜΑΤΟΣ ΜΕ ΧΡΗΣΗ ΤΟΥ ΜΟΝΤΕΛΟΥ BERT**\n",
        "\n",
        "---\n",
        "Αρχικά εισάγωνται οι απαραιτητες βιβλιοθήκες.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwd_HrbyaSBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "#/usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdIPkvNqIaxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mErhZd3asX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFcHFrobOO6m",
        "colab_type": "text"
      },
      "source": [
        "Χωρίζουμε τα δεδομένα μας για την εκπαιδευση και αξιολόγηση του μοντέλου: 80% για την εκπαίδευση και το υπόλοιπο 20% για την αξιολογηση του."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EROVzKkIEtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train,test=train_test_split(df_under,test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = \"Review\"\n",
        "LABEL_COLUMN = \"Polarity\"\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKWL9dLrKzzc",
        "colab_type": "text"
      },
      "source": [
        "Πρέπει το δείγμα μας να έρθει στην μορφή που το μοντέλο BERT \"καταλαβαίνει\". Προς αυτή την κατεύθυνση λοιπόν δημιουργούμε τα train_InputExamples και test_InputExamples χρησιμοποιώντας τον κατασκευαστή που παρέχεται στην βιβλιοθήκη του μοντέλου.\n",
        "\n",
        "\n",
        "*   text_a = είναι το δείγμα κειμένου που θέλουμε να κατηγοριοποιήσουμε\n",
        "*   text_b = χρησιμοποιείται όταν στόχος είναι ο προσδιορισμός σχέσεων που υπάρχουν ανάμεσα στις προτάσεις, κάτι που δεν μας αφορά στην περιπτωσή μας άρα και αφήνεται κενό.\n",
        "*   label = οι κατηγορίες που προσδιορίσαμε είναι οι 0 και 1 που αποδίδουν την αρνητική και θετική κριτική αντίστοιχα.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RJ-Slh9uOVl",
        "colab_type": "text"
      },
      "source": [
        "Το μοντέλο που χρησιμοποιείται είναι το BERT-Base, CASE (12-layer, 768-hidden, 12-heads, 110M). Τα δεδομένα μας τελικά πρέπει να επεξεργαστούν για να έχουν και την μορφή στην οποία έχει εκπαιδευτεί το μοντέλο όπως ήδη αναφέρθηκε παραπάνω. Ετσι ακολουθούν και οι παρακάτω ενέργειες:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  Μετατροπή σε πεζά.\n",
        "*   List it Διαχωρισμός στα κομμάτια της πρότασης (tokenization).\n",
        "*    Διάσπαση λέξεων σε WordPieces (δηλαδή \"calling\" -> [\"call\", \"##ing\"])\n",
        "*   Αντιστοιχίση λέξεων σε ευρετήρια χρησιμοποιώντας το αρχείο που παρεχει το μοντέλο.\n",
        "*    Προσθήκη ειδικών αναγνωριστικών \"CLS\" και \"SEP\".\n",
        "*   Προσθήκη \"index\" και \"segment\" σε κάθε τιμή εισόδου.*   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2W1m6XFxPfUq",
        "colab": {}
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "#WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAl0KNMNG0Wm",
        "colab_type": "text"
      },
      "source": [
        "# **ΔΗΜΙΟΥΡΓΙΑ ΜΟΝΤΕΛΟΥ**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMKzhpM0INJA",
        "colab_type": "text"
      },
      "source": [
        "Στη συνέχεια το μοντέλο ολοκληρώνεται με την μέθοδο model_fn_builder που προσαρμόζει το μοντέλο μας ώστε να μπορεί να εκπαιδευτεί να κάνει αξιολογήσεις καθώς και προβλέψεις."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=\"/content/drive/My Drive/MyThesis/Thesis/Output_dir\",\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Χρησιμοποιούνται εδώ τα δεδομένα που προορίζονται για την αξιολόγηση του μοντέλου."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Ακολουθούν κάποιες προβλέψεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return  [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdQUiJ0eJ6gX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_pred[\"Review\"].head(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx64wFZPUESI",
        "colab_type": "text"
      },
      "source": [
        "Το μοντέλο έχει εκπαιδευτεί και μπορεί πλέον να κάνει προβλέψεις.Ακολουθούν κάποιες προβλέψεις από το δείγμα που απέμεινε από την εξισορρόπηση του δείγματος."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtUBQz-rRVMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_sentences=df_pred[\"Review\"].head(10).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDSrDVd5P99K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_examples=getPrediction(predict_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm3DVWxSkoP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(predictions_examples, columns=['Review','Prediction_Probabilities','Predicted_label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfNJQqnElsi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inner_join_df= pd.merge(df, df_pred.head(10), on='Review', how='inner')\n",
        "inner_join_df = inner_join_df.drop(columns=['Prediction_Probabilities', 'Polarity'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCyML668l7kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(inner_join_df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}